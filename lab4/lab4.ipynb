{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcKalySJTnly",
        "outputId": "d3c2ae80-495b-4b66-f269-24d0920115d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26 26\n",
            "tokenizer.word_index {'the': 1, 'of': 2, 'was': 3, 'it': 4, 'a': 5, 'at': 6, 'and': 7, 'be': 8, 'she': 9, 'in': 10, 'when': 11, 'that': 12, 'so': 13, 'but': 14, 'to': 15, 'have': 16, 'i': 17, 'he': 18, 'is': 19, 'half': 20, 'or': 21, 'this': 22, 'feel': 23, 'understood': 24, 'reason': 25, 'one': 26, 'would': 27, 'life': 28, 'for': 29, 'there': 30, 'days': 31, 'body': 32, 'never': 33, 'out': 34, 'who': 35, 'around': 36, 'about': 37, 'yesterday': 38, 'as': 39, 'good': 40, 'you': 41, 'must': 42, 'really': 43, 'amazing': 44, \"couldn't\": 45, 'decide': 46, 'glass': 47, 'empty': 48, 'full': 49, 'drank': 50, 'made': 51, 'him': 52, 'like': 53, 'an': 54, 'old': 55, 'style': 56, 'rootbeer': 57, 'float': 58, 'smells': 59, 'minute': 60, 'landed': 61, 'fly': 62, 'over': 63, 'state': 64, 'homesickness': 65, 'became': 66, 'contagious': 67, 'young': 68, \"campers'\": 69, 'cabin': 70, 'father': 71, 'handed': 72, 'each': 73, 'child': 74, 'roadmap': 75, 'beginning': 76, '2': 77, 'day': 78, 'road': 79, 'trip': 80, 'explained': 81, 'they': 82, 'could': 83, 'find': 84, 'their': 85, 'way': 86, 'home': 87, 'small': 88, 'action': 89, 'change': 90, 'her': 91, 'whether': 92, 'better': 93, 'worse': 94, 'yet': 95, 'determined': 96, 'stranger': 97, 'officiates': 98, 'meal': 99, 'been': 100, 'wished': 101, 'separated': 102, 'from': 103, 'my': 104, 'today': 105, 'wasn’t': 106, 'those': 107, 'why': 108, 'what': 109, 'where': 110, 'left': 111, 'nobody': 112, 'trees': 113, 'gossip': 114, 'people': 115, 'walked': 116, 'under': 117, 'them': 118, 'lucifer': 119, 'surprised': 120, 'amount': 121, 'death': 122, 'valley': 123, 'surprise': 124, 'everyone': 125, 'rapture': 126, 'happened': 127, \"didn't\": 128, 'quite': 129, 'go': 130, 'expected': 131, \"there's\": 132, 'no': 133, 'hula': 134, 'hoop': 135, \"can't\": 136, 'also': 137, 'circus': 138, 'ring': 139, 'eating': 140, 'pickles': 141, 'telling': 142, 'women': 143, 'his': 144, 'emotional': 145, 'problems': 146, 'doll': 147, 'spun': 148, 'circles': 149, 'hopes': 150, 'coming': 151, 'alive': 152, 'grape': 153, 'jelly': 154, 'leaking': 155, 'hole': 156, 'roof': 157, 'blinking': 158, 'lights': 159, 'antenna': 160, 'tower': 161, 'came': 162, 'into': 163, 'focus': 164, 'just': 165, 'heard': 166, 'loud': 167, 'snap': 168, 'moment': 169, 'learned': 170, 'are': 171, 'certain': 172, 'parts': 173, 'should': 174, 'nair': 175, 'five': 176, \"o'clock\": 177, 'somewhere': 178, 'food': 179, 'oh': 180, 'felt': 181, 'bad': 182, 'do': 183, 'think': 184, \"it's\": 185, 'green': 186, 'sunny': 187, 'its': 188, 'happy': 189, 'shiny': 190, 'hungry': 191, 'alright': 192, 'wonderful': 193}\n",
            "sequences: [[9, 45, 46, 2, 1, 47, 3, 20, 48, 21, 20, 49, 13, 9, 50, 4], [22, 51, 52, 23, 53, 54, 55, 56, 57, 58, 59], [1, 60, 9, 61, 9, 24, 1, 25, 22, 3, 5, 62, 63, 64], [65, 66, 67, 10, 1, 68, 69, 70], [1, 71, 72, 73, 74, 5, 75, 6, 1, 76, 2, 1, 77, 78, 79, 80, 7, 81, 4, 3, 13, 82, 83, 84, 85, 86, 87], [26, 88, 89, 27, 90, 91, 28, 14, 92, 4, 27, 8, 29, 93, 21, 29, 94, 3, 95, 15, 8, 96], [1, 97, 98, 1, 99], [30, 16, 100, 31, 11, 17, 101, 15, 8, 102, 103, 104, 32, 14, 105, 106, 26, 2, 107, 31], [18, 33, 24, 108, 109, 11, 7, 110, 111, 34, 35], [11, 112, 19, 36, 1, 113, 114, 37, 1, 115, 35, 16, 116, 117, 118], [119, 3, 120, 6, 1, 121, 2, 28, 6, 122, 123], [15, 1, 124, 2, 125, 1, 126, 127, 38, 14, 4, 128, 129, 130, 39, 131], [132, 133, 25, 5, 134, 135, 136, 137, 8, 5, 138, 139], [18, 19, 40, 6, 140, 141, 7, 142, 143, 37, 144, 145, 146], [1, 147, 148, 36, 10, 149, 10, 150, 2, 151, 152], [153, 154, 3, 155, 34, 1, 156, 10, 1, 157], [1, 158, 159, 2, 1, 160, 161, 162, 163, 164, 165, 39, 17, 166, 5, 167, 168], [4, 3, 6, 12, 169, 12, 18, 170, 30, 171, 172, 173, 2, 1, 32, 12, 41, 174, 33, 175], [4, 42, 8, 176, 177, 178], [1, 179, 19, 43, 40], [180, 12, 42, 16, 181, 182], [183, 41, 184, 13], [11, 185, 186, 7, 187, 188, 189, 7, 190], [44, 44], [17, 23, 43, 191], [38, 3, 192, 193]]\n",
            "padded sequences: [[  0   0   0   0   0   0   0   0   0   0   0   9  45  46   2   1  47   3\n",
            "   20  48  21  20  49  13   9  50   4]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  22  51\n",
            "   52  23  53  54  55  56  57  58  59]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   1  60   9  61   9\n",
            "   24   1  25  22   3   5  62  63  64]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0  65  66  67  10   1  68  69  70]\n",
            " [  1  71  72  73  74   5  75   6   1  76   2   1  77  78  79  80   7  81\n",
            "    4   3  13  82  83  84  85  86  87]\n",
            " [  0   0   0   0   0  26  88  89  27  90  91  28  14  92   4  27   8  29\n",
            "   93  21  29  94   3  95  15   8  96]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   1  97  98   1  99]\n",
            " [  0   0   0   0   0   0   0  30  16 100  31  11  17 101  15   8 102 103\n",
            "  104  32  14 105 106  26   2 107  31]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  18  33\n",
            "   24 108 109  11   7 110 111  34  35]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0  11 112  19  36   1 113\n",
            "  114  37   1 115  35  16 116 117 118]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 119   3\n",
            "  120   6   1 121   2  28   6 122 123]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0  15   1 124   2 125   1 126\n",
            "  127  38  14   4 128 129 130  39 131]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 132 133  25\n",
            "    5 134 135 136 137   8   5 138 139]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  18  19  40   6\n",
            "  140 141   7 142 143  37 144 145 146]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1 147\n",
            "  148  36  10 149  10 150   2 151 152]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 153\n",
            "  154   3 155  34   1 156  10   1 157]\n",
            " [  0   0   0   0   0   0   0   0   0   0   1 158 159   2   1 160 161 162\n",
            "  163 164 165  39  17 166   5 167 168]\n",
            " [  0   0   0   0   0   0   0   4   3   6  12 169  12  18 170  30 171 172\n",
            "  173   2   1  32  12  41 174  33 175]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   4  42   8 176 177 178]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   1 179  19  43  40]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0 180  12  42  16 181 182]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0 183  41 184  13]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   11 185 186   7 187 188 189   7 190]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0  44  44]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0  17  23  43 191]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0  38   3 192 193]]\n",
            "Labels: [[0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]]\n",
            "Epoch 1/30\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.0962 - accuracy: 0.4000 - val_loss: 1.1104 - val_accuracy: 0.1667\n",
            "Epoch 2/30\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 1.0898 - accuracy: 0.5500 - val_loss: 1.1162 - val_accuracy: 0.1667\n",
            "Epoch 3/30\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.0832 - accuracy: 0.6500 - val_loss: 1.1226 - val_accuracy: 0.1667\n",
            "Epoch 4/30\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 1.0761 - accuracy: 0.6500 - val_loss: 1.1303 - val_accuracy: 0.3333\n",
            "Epoch 5/30\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 1.0684 - accuracy: 0.6500 - val_loss: 1.1398 - val_accuracy: 0.3333\n",
            "Epoch 6/30\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.0597 - accuracy: 0.5500 - val_loss: 1.1521 - val_accuracy: 0.3333\n",
            "Epoch 7/30\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 1.0503 - accuracy: 0.5000 - val_loss: 1.1686 - val_accuracy: 0.3333\n",
            "Epoch 8/30\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.0403 - accuracy: 0.4500 - val_loss: 1.1898 - val_accuracy: 0.3333\n",
            "Epoch 9/30\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 1.0304 - accuracy: 0.4000 - val_loss: 1.2113 - val_accuracy: 0.3333\n",
            "Epoch 10/30\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.0209 - accuracy: 0.4000 - val_loss: 1.2220 - val_accuracy: 0.3333\n",
            "Epoch 11/30\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.0097 - accuracy: 0.4000 - val_loss: 1.2200 - val_accuracy: 0.3333\n",
            "Epoch 12/30\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.9954 - accuracy: 0.4000 - val_loss: 1.2110 - val_accuracy: 0.3333\n",
            "Epoch 13/30\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.9786 - accuracy: 0.4000 - val_loss: 1.2013 - val_accuracy: 0.3333\n",
            "Epoch 14/30\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.9601 - accuracy: 0.5500 - val_loss: 1.1946 - val_accuracy: 0.3333\n",
            "Epoch 15/30\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.9397 - accuracy: 0.5500 - val_loss: 1.1923 - val_accuracy: 0.3333\n",
            "Epoch 16/30\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.9167 - accuracy: 0.7000 - val_loss: 1.1947 - val_accuracy: 0.3333\n",
            "Epoch 17/30\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.8900 - accuracy: 0.7500 - val_loss: 1.2020 - val_accuracy: 0.3333\n",
            "Epoch 18/30\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.8584 - accuracy: 0.7500 - val_loss: 1.2135 - val_accuracy: 0.3333\n",
            "Epoch 19/30\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.8205 - accuracy: 0.7500 - val_loss: 1.2286 - val_accuracy: 0.3333\n",
            "Epoch 20/30\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.7753 - accuracy: 0.7500 - val_loss: 1.2506 - val_accuracy: 0.3333\n",
            "Epoch 21/30\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.7253 - accuracy: 0.7000 - val_loss: 1.2796 - val_accuracy: 0.3333\n",
            "Epoch 22/30\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.6702 - accuracy: 0.7500 - val_loss: 1.3111 - val_accuracy: 0.3333\n",
            "Epoch 23/30\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.6166 - accuracy: 0.8500 - val_loss: 1.2556 - val_accuracy: 0.3333\n",
            "Epoch 24/30\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.5625 - accuracy: 0.7500 - val_loss: 1.2335 - val_accuracy: 0.3333\n",
            "Epoch 25/30\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.5080 - accuracy: 0.9500 - val_loss: 1.1639 - val_accuracy: 0.3333\n",
            "Epoch 26/30\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.4574 - accuracy: 0.9500 - val_loss: 1.1297 - val_accuracy: 0.3333\n",
            "Epoch 27/30\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.4141 - accuracy: 0.9500 - val_loss: 1.1890 - val_accuracy: 0.5000\n",
            "Epoch 28/30\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.4049 - accuracy: 0.8000 - val_loss: 1.4353 - val_accuracy: 0.3333\n",
            "Epoch 29/30\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.8435 - accuracy: 0.6500 - val_loss: 1.0877 - val_accuracy: 0.3333\n",
            "Epoch 30/30\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.3448 - accuracy: 1.0000 - val_loss: 1.4306 - val_accuracy: 0.1667\n",
            "1/1 [==============================] - 0s 454ms/step\n",
            "[[0.07776671 0.8502398  0.07199351]]\n",
            "Predicted tone: Positive\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Embedding\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "texts = [\n",
        "  \"She couldn't decide of the glass was half empty or half full so she drank it.\", # 2\n",
        "  \"This made him feel like an old-style rootbeer float smells.\", # 0\n",
        "  \"The minute she landed she understood the reason this was a fly-over state.\", # 2\n",
        "  \"Homesickness became contagious in the young campers' cabin.\", # 0\n",
        "  \"The father handed each child a roadmap at the beginning of the 2-day road trip and explained it was so they could find their way home.\", # 1\n",
        "  \"One small action would change her life, but whether it would be for better or for worse was yet to be determined.\", # 1\n",
        "  \"The stranger officiates the meal.\", # 2\n",
        "  \"There have been days when I wished to be separated from my body, but today wasn’t one of those days.\", # 1\n",
        "  \"He never understood why what, when, and where left out who.\", # 2\n",
        "  \"When nobody is around, the trees gossip about the people who have walked under them.\", # 1\n",
        "  \"Lucifer was surprised at the amount of life at Death Valley.\", # 2\n",
        "  \"To the surprise of everyone, the Rapture happened yesterday but it didn't quite go as expected.\", # 0\n",
        "  \"There's no reason a hula hoop can't also be a circus ring.\", # 2\n",
        "  \"He is good at eating pickles and telling women about his emotional problems.\", # 2\n",
        "  \"The doll spun around in circles in hopes of coming alive.\",  # 0\n",
        "  \"Grape jelly was leaking out the hole in the roof.\", # 0\n",
        "  \"The blinking lights of the antenna tower came into focus just as I heard a loud snap.\", # 0\n",
        "  \"It was at that moment that he learned there are certain parts of the body that you should never Nair.\", # 2\n",
        "  \"It must be five o'clock somewhere.\", # 2\n",
        "  \"The food is really good!\", # 1\n",
        "  \"Oh that must have felt bad..\", # 0\n",
        "  \"Do you think so?\", # 2\n",
        "  \"When it's green and sunny its happy and shiny\", # 1\n",
        "  \"Amazing? Amazing..\", # 1\n",
        "  \"I feel really hungry..\", # 0\n",
        "  \"Yesterday was alright, Wonderful!\" # 1\n",
        "]\n",
        "\n",
        "labels = [\n",
        "    2, 0, 2, 0, 1, 1, 2, 1, 2, 1, 2, 0, 2, 2, 0, 0, 0, 2, 2, 1, 0, 2, 1, 1, 0, 1\n",
        "]\n",
        "\n",
        "# 0 = negative\n",
        "# 1 = positive\n",
        "# 2 = neither\n",
        "\n",
        "print(len(labels), len(texts));\n",
        "\n",
        "#for idx, x in enumerate(texts):\n",
        "#  print(x, labels[idx])\n",
        "\n",
        "# tokenizing texts\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "print(f\"tokenizer.word_index {tokenizer.word_index}\")\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "print(f\"sequences: {sequences}\");\n",
        "\n",
        "max_sequence_length = max([len(seq) for seq in sequences])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "print(f\"padded sequences: {padded_sequences}\")\n",
        "\n",
        "labels = np.eye(3)[labels]\n",
        "print(f\"Labels: {labels}\")\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "embedding_dim = 50\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length))\n",
        "model.add(LSTM(units=100))\n",
        "model.add(Dense(units=3, activation=\"softmax\"))\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=30, batch_size=32, validation_data=(x_test, y_test))\n",
        "\n",
        "def predict_tone(text):\n",
        "  sequence = tokenizer.texts_to_sequences([text])\n",
        "  padded_sequence = pad_sequences(sequence, maxlen=max_sequence_length)\n",
        "  prediction = model.predict(padded_sequence)\n",
        "  print(prediction)\n",
        "  tone = np.argmax(prediction)\n",
        "  if tone == 0:\n",
        "    return \"Negative\"\n",
        "  elif tone == 1:\n",
        "    return \"Positive\"\n",
        "  else:\n",
        "    return \"Neither\"\n",
        "\n",
        "test_text = \"This is sad..\"\n",
        "print(f\"Predicted tone: {predict_tone(test_text)}\")\n",
        "# if you are reading this, then.. hello again! :)\n",
        "\n",
        "# looks like nothing is working.. but i tried!\n",
        "# multiple attempts of changing texts/labels/everything have been made yet nothing works\n",
        "# maybe for the neural system its not too obvious and i had to find/write sentences that were more explicit"
      ]
    }
  ]
}